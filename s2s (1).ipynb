{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "s2s.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2jLhVOEeYr4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrj00wasd0xh"
      },
      "source": [
        "#2 Assignment\n",
        "from keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnPLdI0id3Ga"
      },
      "source": [
        "print(\"Categories:\", np.unique(targets))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))\n",
        "\n",
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] )\n",
        "print(decoded) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBnOHHIpd3Lg"
      },
      "source": [
        "def vectorize(sequences, dimension = 10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "       results[i, sequence] = 1\n",
        "    return results\n",
        " \n",
        "data = vectorize(data)\n",
        "targets = np.array(targets).astype(\"float32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCBFwupyd3NI"
      },
      "source": [
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrHDLDg8d3QC"
      },
      "source": [
        "from tensorflow.keras.constraints import UnitNorm\n",
        "from tensorflow.keras.constraints import Constraint\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "\n",
        "input_dim = (train_x.shape[1])\n",
        "\n",
        "encoder = Dense (units = 256 ,activation =\"relu\",input_shape =( input_dim ,) ,use_bias = True , \n",
        "                  kernel_constraint =UnitNorm ( axis =0) ,activity_regularizer =tf.keras.regularizers.L1( l1 =0.01),name ='encoder')\n",
        "decoder= Dense (units = 1 ,activation =\"linear\",use_bias = True , kernel_constraint =UnitNorm ( axis =1),name ='decoder')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWHnonHmd3Ry"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(encoder)\n",
        "model.add(decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTx77qDfd3Un"
      },
      "source": [
        "model.summary ()\n",
        "model.compile ( metrics =[ 'accuracy'] ,loss ='mean_squared_error',optimizer ='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czcSg8Ujd3W1"
      },
      "source": [
        "history =  model. fit (x = train_x ,y = train_y ,batch_size =32 ,epochs =10 ,validation_data =( test_x ,test_y ) ,verbose =0).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk8-gbOLeN0I"
      },
      "source": [
        "encoder_model =tf.keras.Model ( inputs = model . inputs ,outputs = model. get_layer ('encoder'). output )\n",
        "X_train_encoded_features = encoder_model . predict ( train_x )\n",
        "#X_valid_encoded_features = encoder_model . predict ( test_x )\n",
        "X_test_encoded_features =encoder_model . predict ( test_x )\n",
        "\n",
        " # Model\n",
        "classifier = Sequential ()\n",
        "classifier . add ( tf.keras.Input (shape =( X_train_encoded_features . shape [1] , ) ) )\n",
        "classifier . add ( Dense ( units =32 ,activation ='relu') )\n",
        "classifier . add ( Dense ( units =16 ,activation ='relu') )\n",
        "classifier . add ( Dense ( units =1 ,activation ='sigmoid') )\n",
        "classifier . compile ( optimizer ='adam',loss ='binary_crossentropy',metrics =[ 'accuracy' ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI0D-BBdeTzR"
      },
      "source": [
        "history = classifier . fit (x = X_train_encoded_features ,y = train_y ,batch_size =128 ,epochs =10 ,validation_data =( X_valid_encoded_features ,test_y ) ,verbose =0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhedfZ_BeUoS"
      },
      "source": [
        "#list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}